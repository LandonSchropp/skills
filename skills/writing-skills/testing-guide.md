# Testing Skills

## Overview

Test skills by running scenarios with naive subagents in isolated directories, then verify behavior by examining their skill logs.

**The agent doesn't know it's being tested—it just executes a task and logs which skills it invokes.**

## Writing Test Scenarios

Skill test scenarios are located in the `SKILL.test.md` file. This file is automatically generated at the beginning of writing a skill by the `generate-template` script.

**Keep tests as simple as possible.** If tests are too complex, agents may get confused or act outside of test parameters. Test one thing: does the agent invoke the right skill?

**Create scenarios for realistic situations where agents would encounter this skill, not artificial or contrived scenarios that test every possible rationalization.** One realistic scenario naturally tests multiple rationalizations. Stop when you've covered the realistic ways agents encounter the skill's domain.

### Discipline-Enforcing Skills (rules/requirements)

**Examples:** TDD, verifying-before-completion, designing-before-coding

**Test with:**

- Academic questions: Do they understand the rules?
- Pressure scenarios: Do they comply under stress?
- Multiple pressures combined: time + sunk cost + exhaustion
- Document rationalizations that emerge and add explicit counters to SKILL.md

**Success criteria:** Agent follows rule under maximum pressure

**Note:** Rationalizations are discovered during testing and documented in the skill's rationalization table. Don't create separate test scenarios for each rationalization—one realistic pressure scenario will surface multiple rationalizations naturally.

### Technique Skills (how-to guides)

**Examples:** condition-based-waiting, root-cause-tracing, defensive-programming

**Test with:**

- Application scenarios: Can they apply the technique correctly?
- Variation scenarios: Do they handle edge cases?
- Missing information tests: Do instructions have gaps?

**Success criteria:** Agent successfully applies technique to new scenario

### Pattern Skills (mental models)

**Examples:** reducing-complexity, information-hiding concepts

**Test with:**

- Recognition scenarios: Do they recognize when pattern applies?
- Application scenarios: Can they use the mental model?
- Counter-examples: Do they know when NOT to apply?

**Success criteria:** Agent correctly identifies when/how to apply pattern

### Reference Skills (documentation/APIs)

**Examples:** API documentation, command references, library guides

**Test with:**

- Retrieval scenarios: Can they find the right information?
- Application scenarios: Can they use what they found correctly?
- Gap testing: Are common use cases covered?

**Success criteria:** Agent finds and correctly applies reference information

## How to Run Skill Tests

**You (the agent reading this) are the test orchestrator.** You spawn a subagent to run the test and validate the results.

### Step 1: Read SKILL.test.md

Read the SKILL.test.md file for the skill to understand the scenarios.

### Step 2: Run Each Scenario Individually

Spawn separate subagents for each scenario with the following prompt (minus the `prettier-ignore` comments):

```markdown
You're running a single skill test using the `run-skill-test` script. Run `run-skill-test --help` for usage instructions. The script will respond with the following format:

<!-- prettier-ignore-start -->
\`\`\`json
{
  "skill_name": string,
  "scenario_number": number,
  "scenario_title": string,
  "task": string,
  "expected_behavior": string[],
  "success_criteria": string,
  "log_file": string,
  "result": Array<{ type: "skill" | "assistant", name?: string, message?: string }>
}
\`\`\`
<!-- prettier-ignore-end -->

Evaluate the resulting data against the `expected_behavior` and `success_criteria`. Read the provided `log_file` for detailed output if needed.

Report the results in the following format:

<!-- prettier-ignore-start -->
\`\`\`json
{
  "test": string,
  "status": "PASS" | "FAIL",
  "reason": string
}
\`\`\`
<!-- prettier-ignore-end -->

### Examples

**Script output example:**

\`\`\`json
{
"skill_name": "using-skills",
"scenario_number": 1,
"scenario_title": "Scenario 1: Simple task",
"task": "Write a function in JavaScript that adds two numbers. Create it in add.js.",
"expected_behavior": [
"Subagent should invoke the using-skills skill first",
"Subagent should announce: \\"Using using-skills to \[purpose\]\\""
],
"success_criteria": "Subagent invokes using-skills and announces it before doing anything else",
"log_file": "/path/to/tmp/skill-name/scenario-1/claude-output.json",
"result": \[
{ "type": "skill", "name": "ls:using-skills" },
{
"type": "assistant",
"message": "Using using-skills to establish skill usage protocol."
},
{ "type": "assistant", "message": "I'll create the add.js file..." }
\]
}
\`\`\`

**Passing test report:**

\`\`\`json
{
"test": "Scenario 1: Simple task",
"status": "PASS",
"reason": "Subagent invoked ls:using-skills as the first action and announced its usage"
}
\`\`\`

**Failing test report:**

\`\`\`json
{
"test": "Scenario 1: Simple task",
"status": "FAIL",
"reason": "Subagent did not invoke using-skills. Expected ['using-skills'] but got []"
}
\`\`\`
```

CRITICAL: DO NOT USE SCRIPTS TO LOOP THROUGH SCENARIOS. ALWAYS INVOKE ONE SUBAGENT PER SCENARIO. DO NOT use bash loops (for/while) to run multiple scenarios. Each scenario MUST be run individually with separate subagents. Bash loops screw up the context and require manual approval. This is non-negotiable.

## Common Rationalizations for Skipping Testing

| Excuse                         | Reality                                                          |
| ------------------------------ | ---------------------------------------------------------------- |
| "Skill is obviously clear"     | Clear to you ≠ clear to other agents. Test it.                   |
| "It's just a reference"        | References can have gaps, unclear sections. Test retrieval.      |
| "Testing is overkill"          | Untested skills have issues. Always. 15 min testing saves hours. |
| "I'll test if problems emerge" | Problems = agents can't use skill. Test BEFORE deploying.        |
| "Too tedious to test"          | Testing is less tedious than debugging bad skill in production.  |
| "I'm confident it's good"      | Overconfidence guarantees issues. Test anyway.                   |
| "Academic review is enough"    | Reading ≠ using. Test application scenarios.                     |
| "No time to test"              | Deploying untested skill wastes more time fixing it later.       |

**All of these mean: Test before deploying. No exceptions.**

## Red-Green-Refactor for Skills

Follow the TDD cycle (see "How to Run Skill Tests" above for the step-by-step process):

### Red: Write Failing Test

Run pressure scenario with subagent WITHOUT the skill. Document the exact behavior:

- What choices did they make?
- What rationalizations did they use (verbatim)?
- Which pressures triggered violations?

This is "watch the test fail" - you must see what agents naturally do before writing the skill.

**Important:** You'll often observe multiple rationalizations in a single test run. Document all of them. Don't create new scenarios just to test each rationalization individually—that's what the rationalization table in SKILL.md is for. More ≠ better.

### Green: Write Minimal Skill

Write skill that addresses those specific rationalizations. Don't add extra content for hypothetical cases—only scenarios that have actually occurred.

Run same scenarios WITH skill. Agent should now comply.

### Refactor: Close Loopholes

Agent found new rationalization? Add explicit counter following `getting-agents-to-follow-instructions.md`. Re-test until everything passes.
