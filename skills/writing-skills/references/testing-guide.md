# Testing Skills

## Overview

Test skills by running scenarios with naive subagents in isolated directories, then verify behavior by examining their skill logs.

**The agent doesn't know it's being tested—it just executes a task and logs which skills it invokes.**

## Writing Test Scenarios

Skill test scenarios are located in the `SKILL.test.md` file. This file is automatically generated at the beginning of writing a skill by the `generate-template` script.

**Keep tests as simple as possible.** If tests are too complex, agents may get confused or act outside of test parameters. Test one thing: does the agent invoke the right skill?

**Create scenarios for realistic situations where agents would encounter this skill, not artificial or contrived scenarios that test every possible rationalization.** One realistic scenario naturally tests multiple rationalizations. Stop when you've covered the realistic ways agents encounter the skill's domain.

### Discipline-Enforcing Skills (rules/requirements)

**Examples:** TDD, verifying-before-completion, designing-before-coding

**Test with:**

- Academic questions: Do they understand the rules?
- Pressure scenarios: Do they comply under stress?
- Multiple pressures combined: time + sunk cost + exhaustion
- Document rationalizations that emerge and add explicit counters to SKILL.md

**Success criteria:** Agent follows rule under maximum pressure

**Note:** Rationalizations are discovered during testing and documented in the skill's rationalization table. Don't create separate test scenarios for each rationalization—one realistic pressure scenario will surface multiple rationalizations naturally.

### Technique Skills (how-to guides)

**Examples:** condition-based-waiting, root-cause-tracing, defensive-programming

**Test with:**

- Application scenarios: Can they apply the technique correctly?
- Variation scenarios: Do they handle edge cases?
- Missing information tests: Do instructions have gaps?

**Success criteria:** Agent successfully applies technique to new scenario

### Pattern Skills (mental models)

**Examples:** reducing-complexity, information-hiding concepts

**Test with:**

- Recognition scenarios: Do they recognize when pattern applies?
- Application scenarios: Can they use the mental model?
- Counter-examples: Do they know when NOT to apply?

**Success criteria:** Agent correctly identifies when/how to apply pattern

### Reference Skills (documentation/APIs)

**Examples:** API documentation, command references, library guides

**Test with:**

- Retrieval scenarios: Can they find the right information?
- Application scenarios: Can they use what they found correctly?
- Gap testing: Are common use cases covered?

**Success criteria:** Agent finds and correctly applies reference information

## How to Run Skill Tests

Run the script with the skill's SKILL.test.yml file:

```bash
scripts/evaluate-skill.ts --yamlFile ../<skill-name>/SKILL.test.yml
```

Run `scripts/evaluate-skill.ts --help` for a list of the parameters.

## Common Rationalizations for Skipping Testing

| Excuse                         | Reality                                                          |
| ------------------------------ | ---------------------------------------------------------------- |
| "Skill is obviously clear"     | Clear to you ≠ clear to other agents. Test it.                   |
| "It's just a reference"        | References can have gaps, unclear sections. Test retrieval.      |
| "Testing is overkill"          | Untested skills have issues. Always. 15 min testing saves hours. |
| "I'll test if problems emerge" | Problems = agents can't use skill. Test BEFORE deploying.        |
| "Too tedious to test"          | Testing is less tedious than debugging bad skill in production.  |
| "I'm confident it's good"      | Overconfidence guarantees issues. Test anyway.                   |
| "Academic review is enough"    | Reading ≠ using. Test application scenarios.                     |
| "No time to test"              | Deploying untested skill wastes more time fixing it later.       |

**All of these mean: Test before deploying. No exceptions.**

## Red-Green-Refactor for Skills

Follow the TDD cycle (see "How to Run Skill Tests" above for the step-by-step process):

### Red: Write Failing Test

Run pressure scenario with subagent WITHOUT the skill. Document the exact behavior:

- What choices did they make?
- What rationalizations did they use (verbatim)?
- Which pressures triggered violations?

This is "watch the test fail" - you must see what agents naturally do before writing the skill.

**Important:** You'll often observe multiple rationalizations in a single test run. Document all of them. Don't create new scenarios just to test each rationalization individually—that's what the rationalization table in SKILL.md is for. More ≠ better.

### Green: Write Minimal Skill

Write skill that addresses those specific rationalizations. Don't add extra content for hypothetical cases—only scenarios that have actually occurred.

Run same scenarios WITH skill. Agent should now comply.

### Refactor: Close Loopholes

Agent found new rationalization? Add explicit counter following `references/getting-agents-to-follow-instructions.md`. Re-test until everything passes.
