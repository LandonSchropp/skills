# Testing Skills

## Overview

Test skills by running scenarios with naive subagents in isolated directories, then verify behavior by examining their skill logs.

**The agent doesn't know it's being tested—it just executes a task and logs which skills it invokes.**

## Writing Test Scenarios

Skill test scenarios are located in the `SKILL.test.md` file. This file is automatically generated at the beginning of writing a skill by the `generate-template` script.

**Keep tests as simple as possible.** If tests are too complex, agents may get confused or act outside of test parameters. Test one thing: does the agent invoke the right skill?

### Discipline-Enforcing Skills (rules/requirements)

**Examples:** TDD, verification-before-completion, designing-before-coding

**Test with:**

- Academic questions: Do they understand the rules?
- Pressure scenarios: Do they comply under stress?
- Multiple pressures combined: time + sunk cost + exhaustion
- Identify rationalizations and add explicit counters

**Success criteria:** Agent follows rule under maximum pressure

### Technique Skills (how-to guides)

**Examples:** condition-based-waiting, root-cause-tracing, defensive-programming

**Test with:**

- Application scenarios: Can they apply the technique correctly?
- Variation scenarios: Do they handle edge cases?
- Missing information tests: Do instructions have gaps?

**Success criteria:** Agent successfully applies technique to new scenario

### Pattern Skills (mental models)

**Examples:** reducing-complexity, information-hiding concepts

**Test with:**

- Recognition scenarios: Do they recognize when pattern applies?
- Application scenarios: Can they use the mental model?
- Counter-examples: Do they know when NOT to apply?

**Success criteria:** Agent correctly identifies when/how to apply pattern

### Reference Skills (documentation/APIs)

**Examples:** API documentation, command references, library guides

**Test with:**

- Retrieval scenarios: Can they find the right information?
- Application scenarios: Can they use what they found correctly?
- Gap testing: Are common use cases covered?

**Success criteria:** Agent finds and correctly applies reference information

## How to Run Skill Tests

**You (the agent reading this) are the test orchestrator.** You spawn a subagent to run the test and validate the results.

### Step 1: Read SKILL.test.md

Read the SKILL.test.md file for the skill to understand the scenarios.

### Step 2: Run Subagent

Spawn subagents for each scenario with the following prompt (minus the `prettier-ignore` comments):

```markdown
You're running a single skill test. Run `run-skill-test <skill-directory> <scenario-number>`. The script will respond with the following format:

<!-- prettier-ignore-start -->
\`\`\`json
{
  "skill_name": string,
  "scenario_number": number,
  "scenario_title": string,
  "task": string,
  "expected_behavior": string[],
  "success_criteria": string,
  "log_file": string,
  "result": Array<{ type: "skill" | "assistant", name?: string, message?: string }>
}
\`\`\`
<!-- prettier-ignore-end -->

Evaluate the resulting data against the `expected_behavior` and `success_criteria`. Read the provided `log_file` for detailed output if needed.

Report the results in the following format:

<!-- prettier-ignore-start -->
\`\`\`json
{
  "test": string,
  "status": "PASS" | "FAIL",
  "reason": string
}
\`\`\`
<!-- prettier-ignore-end -->

### Examples

**Script output example:**

\`\`\`json
{
"skill_name": "skill-instructions",
"scenario_number": 1,
"scenario_title": "Scenario 1: Simple task",
"task": "Write a function in JavaScript that adds two numbers. Create it in add.js.",
"expected_behavior": [
"Subagent should invoke the skill-instructions skill first",
"Subagent should announce: \\"Using skill-instructions to \[purpose\]\\""
],
"success_criteria": "Subagent invokes skill-instructions and announces it before doing anything else",
"log_file": "/path/to/tmp/skill-name/scenario-1/claude-output.json",
"result": \[
{ "type": "skill", "name": "ls:skill-instructions" },
{
"type": "assistant",
"message": "Using skill-instructions to establish skill usage protocol."
},
{ "type": "assistant", "message": "I'll create the add.js file..." }
\]
}
\`\`\`

**Passing test report:**

\`\`\`json
{
"test": "Scenario 1: Simple task",
"status": "PASS",
"reason": "Subagent invoked ls:skill-instructions as the first action and announced its usage"
}
\`\`\`

**Failing test report:**

\`\`\`json
{
"test": "Scenario 1: Simple task",
"status": "FAIL",
"reason": "Subagent did not invoke skill-instructions. Expected ['skill-instructions'] but got []"
}
\`\`\`
```

## Common Rationalizations for Skipping Testing

| Excuse                         | Reality                                                          |
| ------------------------------ | ---------------------------------------------------------------- |
| "Skill is obviously clear"     | Clear to you ≠ clear to other agents. Test it.                   |
| "It's just a reference"        | References can have gaps, unclear sections. Test retrieval.      |
| "Testing is overkill"          | Untested skills have issues. Always. 15 min testing saves hours. |
| "I'll test if problems emerge" | Problems = agents can't use skill. Test BEFORE deploying.        |
| "Too tedious to test"          | Testing is less tedious than debugging bad skill in production.  |
| "I'm confident it's good"      | Overconfidence guarantees issues. Test anyway.                   |
| "Academic review is enough"    | Reading ≠ using. Test application scenarios.                     |
| "No time to test"              | Deploying untested skill wastes more time fixing it later.       |

**All of these mean: Test before deploying. No exceptions.**

## Red-Green-Refactor for Skills

Follow the TDD cycle (see "How to Run Skill Tests" above for the step-by-step process):

### Red: Write Failing Test

Run pressure scenario with subagent WITHOUT the skill. Document the exact behavior:

- What choices did they make?
- What rationalizations did they use (verbatim)?
- Which pressures triggered violations?

This is "watch the test fail" - you must see what agents naturally do before writing the skill.

### Green: Write Minimal Skill

Write skill that addresses those specific rationalizations. Don't add extra content for hypothetical cases—only scenarios that have actually occurred.

Run same scenarios WITH skill. Agent should now comply.

### Refactor: Close Loopholes

Agent found new rationalization? Add explicit counter following `getting-agents-to-follow-instructions.md`. Re-test until everything passes.
